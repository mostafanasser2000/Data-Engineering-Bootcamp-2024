{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_53380/4172481980.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fhv_tripdata_2019-10.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dispatching_base_num', 'pickup_datetime', 'dropOff_datetime',\n",
       "       'PUlocationID', 'DOlocationID', 'SR_Flag', 'Affiliated_base_number'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatching_base_num       object\n",
      "pickup_datetime            object\n",
      "dropOff_datetime           object\n",
      "PUlocationID              float64\n",
      "DOlocationID              float64\n",
      "SR_Flag                   float64\n",
      "Affiliated_base_number     object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dispatching_base_num</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropOff_datetime</th>\n",
       "      <th>PUlocationID</th>\n",
       "      <th>DOlocationID</th>\n",
       "      <th>SR_Flag</th>\n",
       "      <th>Affiliated_base_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00009</td>\n",
       "      <td>2019-10-01 00:23:00</td>\n",
       "      <td>2019-10-01 00:35:00</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00013</td>\n",
       "      <td>2019-10-01 00:11:29</td>\n",
       "      <td>2019-10-01 00:13:22</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:11:43</td>\n",
       "      <td>2019-10-01 00:37:20</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:56:29</td>\n",
       "      <td>2019-10-01 00:57:47</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00014</td>\n",
       "      <td>2019-10-01 00:23:09</td>\n",
       "      <td>2019-10-01 00:28:27</td>\n",
       "      <td>264.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00021</td>\n",
       "      <td>2019-10-01 00:00:48</td>\n",
       "      <td>2019-10-01 00:07:12</td>\n",
       "      <td>129.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00021</td>\n",
       "      <td>2019-10-01 00:47:23</td>\n",
       "      <td>2019-10-01 00:53:25</td>\n",
       "      <td>57.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00021</td>\n",
       "      <td>2019-10-01 00:10:06</td>\n",
       "      <td>2019-10-01 00:19:50</td>\n",
       "      <td>173.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00021</td>\n",
       "      <td>2019-10-01 00:51:37</td>\n",
       "      <td>2019-10-01 01:06:14</td>\n",
       "      <td>226.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00021</td>\n",
       "      <td>2019-10-01 00:28:23</td>\n",
       "      <td>2019-10-01 00:34:33</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B00021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dispatching_base_num      pickup_datetime     dropOff_datetime  \\\n",
       "0               B00009  2019-10-01 00:23:00  2019-10-01 00:35:00   \n",
       "1               B00013  2019-10-01 00:11:29  2019-10-01 00:13:22   \n",
       "2               B00014  2019-10-01 00:11:43  2019-10-01 00:37:20   \n",
       "3               B00014  2019-10-01 00:56:29  2019-10-01 00:57:47   \n",
       "4               B00014  2019-10-01 00:23:09  2019-10-01 00:28:27   \n",
       "5      B00021           2019-10-01 00:00:48  2019-10-01 00:07:12   \n",
       "6      B00021           2019-10-01 00:47:23  2019-10-01 00:53:25   \n",
       "7      B00021           2019-10-01 00:10:06  2019-10-01 00:19:50   \n",
       "8      B00021           2019-10-01 00:51:37  2019-10-01 01:06:14   \n",
       "9      B00021           2019-10-01 00:28:23  2019-10-01 00:34:33   \n",
       "\n",
       "   PUlocationID  DOlocationID  SR_Flag Affiliated_base_number  \n",
       "0         264.0         264.0      NaN                 B00009  \n",
       "1         264.0         264.0      NaN                 B00013  \n",
       "2         264.0         264.0      NaN                 B00014  \n",
       "3         264.0         264.0      NaN                 B00014  \n",
       "4         264.0         264.0      NaN                 B00014  \n",
       "5         129.0         129.0      NaN        B00021           \n",
       "6          57.0          57.0      NaN        B00021           \n",
       "7         173.0         173.0      NaN        B00021           \n",
       "8         226.0         226.0      NaN        B00021           \n",
       "9          56.0          56.0      NaN        B00021           "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "        types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"dropOff_datetime\", types.TimestampType(), True),\n",
    "        types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"DOlocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"SR_Flag\", types.StringType(), True),\n",
    "        types.StructField(\"Affiliated_base_number\", types.StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_df = spark.read.option(\"header\", \"true\").schema(fhv_schema).csv('fhv_tripdata_2019-10.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PULocationID|DOlocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00009|2019-10-01 00:23:00|2019-10-01 00:35:00|         264|         264|   null|                B00009|\n",
      "|              B00013|2019-10-01 00:11:29|2019-10-01 00:13:22|         264|         264|   null|                B00013|\n",
      "|              B00014|2019-10-01 00:11:43|2019-10-01 00:37:20|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:56:29|2019-10-01 00:57:47|         264|         264|   null|                B00014|\n",
      "|              B00014|2019-10-01 00:23:09|2019-10-01 00:28:27|         264|         264|   null|                B00014|\n",
      "|     B00021         |2019-10-01 00:00:48|2019-10-01 00:07:12|         129|         129|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:47:23|2019-10-01 00:53:25|          57|          57|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:10:06|2019-10-01 00:19:50|         173|         173|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:51:37|2019-10-01 01:06:14|         226|         226|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:28:23|2019-10-01 00:34:33|          56|          56|   null|       B00021         |\n",
      "|     B00021         |2019-10-01 00:31:17|2019-10-01 00:51:52|          82|          82|   null|       B00021         |\n",
      "|              B00037|2019-10-01 00:07:41|2019-10-01 00:15:23|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:13:38|2019-10-01 00:25:51|         264|          39|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:42:40|2019-10-01 00:53:47|         264|         188|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:58:46|2019-10-01 01:10:11|         264|          91|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:09:49|2019-10-01 00:14:37|         264|          71|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:22:35|2019-10-01 00:36:53|         264|          35|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:54:27|2019-10-01 01:03:37|         264|          61|   null|                B00037|\n",
      "|              B00037|2019-10-01 00:08:12|2019-10-01 00:28:47|         264|         198|   null|                B00037|\n",
      "|              B00053|2019-10-01 00:05:24|2019-10-01 00:53:03|         264|         264|   null|                  #N/A|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fhv_df.repartition(6).write.parquet('data/pq/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_data = spark.read.parquet('data/pq/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', TimestampType(), True), StructField('dropOff_datetime', TimestampType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOlocationID', IntegerType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "pprint(fhv_data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Programs/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "fhv_data.registerTempTable('fhv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+\n",
      "|         pickup_day|trip_records|\n",
      "+-------------------+------------+\n",
      "|2019-10-15 00:00:00|       62610|\n",
      "+-------------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "    date_trunc('day', pickup_datetime) AS pickup_day,\n",
    "    COUNT(1) AS trip_records\n",
    "FROM \n",
    "    fhv\n",
    "WHERE \n",
    "    date_trunc('day', pickup_datetime) = DATE '2019-10-15'\n",
    "GROUP BY\n",
    "    date_trunc('day', pickup_datetime);\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|    pickup_datetime|   dropOff_datetime|total_trip_distance|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2019-10-11 18:00:00|2091-10-11 18:30:00|             631152|\n",
      "+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "    pickup_datetime, \n",
    "    dropOff_datetime,\n",
    "    DATEDIFF(HOUR, pickup_datetime, dropOff_datetime) AS total_trip_distance\n",
    "    \n",
    "FROM \n",
    "    fhv\n",
    "ORDER BY total_trip_distance DESC\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LocationID</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "      <th>service_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>EWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>Yellow Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>Arrochar/Fort Wadsworth</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Astoria</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Astoria Park</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Auburndale</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Baisley Park</td>\n",
       "      <td>Boro Zone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LocationID        Borough                     Zone service_zone\n",
       "0           1            EWR           Newark Airport          EWR\n",
       "1           2         Queens              Jamaica Bay    Boro Zone\n",
       "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
       "3           4      Manhattan            Alphabet City  Yellow Zone\n",
       "4           5  Staten Island            Arden Heights    Boro Zone\n",
       "5           6  Staten Island  Arrochar/Fort Wadsworth    Boro Zone\n",
       "6           7         Queens                  Astoria    Boro Zone\n",
       "7           8         Queens             Astoria Park    Boro Zone\n",
       "8           9         Queens               Auburndale    Boro Zone\n",
       "9          10         Queens             Baisley Park    Boro Zone"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones = pd.read_csv('taxi_zone_lookup.csv')\n",
    "df_zones.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LocationID       int64\n",
       "Borough         object\n",
       "Zone            object\n",
       "service_zone    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_zones.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_schema = types.StructType(\n",
    "    [\n",
    "        types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "        types.StructField(\"Borough\", types.StringType(), True),\n",
    "        types.StructField(\"Zone\", types.StringType(), True),\n",
    "        types.StructField(\"service_zone\", types.StringType(), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones = spark.read.option(\"header\", \"true\").schema(zones_schema).csv(\"taxi_zone_lookup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zones.write.parquet('data/pq/zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_data = spark.read.parquet('data/pq/zones/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Programs/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "zones_data.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|pickup_zone|zone_records|\n",
      "+-----------+------------+\n",
      "|Jamaica Bay|           1|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "\n",
    "SELECT Zone AS pickup_zone,\n",
    "    COUNT(1) AS zone_records\n",
    "FROM \n",
    "    fhv\n",
    "JOIN\n",
    "    zones\n",
    "ON\n",
    "    fhv.PULocationID = zones.LocationID\n",
    "\n",
    "GROUP BY\n",
    "    Zone\n",
    "ORDER BY zone_records\n",
    "LIMIT 1\n",
    "\"\"\"\n",
    ").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
